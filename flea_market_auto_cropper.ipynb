{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "id": "cea9257f",
      "cell_type": "markdown",
      "source": [
        "# üß† Flea Market Auto-Cropper & Inventory Matcher\n",
        "This notebook detects, crops, and labels individual items in a cluttered image using Grounding DINO, Segment Anything (SAM), and BLIP. It then matches each item to your custom inventory list based on visual captions."
      ],
      "metadata": {
        "id": "cea9257f"
      }
    },
    {
      "id": "a7a094a1",
      "cell_type": "code",
      "metadata": {
        "id": "a7a094a1"
      },
      "execution_count": null,
      "source": [
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!pip install -q git+https://github.com/salesforce/BLIP.git\n",
        "!pip install -q transformers diffusers timm sentence-transformers opencv-python\n"
      ],
      "outputs": []
    },
    {
      "id": "e480a2cb",
      "cell_type": "code",
      "metadata": {
        "id": "e480a2cb"
      },
      "execution_count": null,
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "print(\"Upload your flea market image:\")\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "print(\"Upload your item list (CSV with title, description, keywords):\")\n",
        "uploaded_csv = files.upload()\n",
        "item_csv_path = list(uploaded_csv.keys())[0]\n"
      ],
      "outputs": []
    },
    {
      "id": "4ae522c2",
      "cell_type": "code",
      "metadata": {
        "id": "4ae522c2"
      },
      "execution_count": null,
      "source": [
        "# üîÅ Load BLIP captioning and semantic matching models\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load BLIP captioning model\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Load SentenceTransformer for semantic matching\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "outputs": []
    },
    {
      "id": "e9279659",
      "cell_type": "code",
      "metadata": {
        "id": "e9279659"
      },
      "execution_count": null,
      "source": [
        "# üîÅ Fake crop logic ‚Äì simulates 4 item crops (until GroundingDINO is added)\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "width, height = img.size\n",
        "cropped_dir = \"crops\"\n",
        "os.makedirs(cropped_dir, exist_ok=True)\n",
        "\n",
        "cropped_images = []\n",
        "for i, (x1, y1, x2, y2) in enumerate([\n",
        "    (0, 0, width//2, height//2),\n",
        "    (width//2, 0, width, height//2),\n",
        "    (0, height//2, width//2, height),\n",
        "    (width//2, height//2, width, height)\n",
        "]):\n",
        "    crop = img.crop((x1, y1, x2, y2))\n",
        "    crop_path = f\"{cropped_dir}/item_{i+1}.jpg\"\n",
        "    crop.save(crop_path)\n",
        "    cropped_images.append((crop_path, crop))\n"
      ],
      "outputs": []
    },
    {
      "id": "d6ccdac5",
      "cell_type": "code",
      "metadata": {
        "id": "d6ccdac5"
      },
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(item_csv_path)\n",
        "\n",
        "matched_data = []\n",
        "\n",
        "def caption_image(image):\n",
        "    inputs = blip_processor(image, return_tensors=\"pt\")\n",
        "    out = blip_model.generate(**inputs)\n",
        "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Compute sentence embeddings for inventory titles\n",
        "df[\"embedding\"] = df[\"title\"].apply(lambda x: semantic_model.encode(x, convert_to_tensor=True))\n",
        "\n",
        "for path, image in cropped_images:\n",
        "    caption = caption_image(image)\n",
        "    cap_embed = semantic_model.encode(caption, convert_to_tensor=True)\n",
        "\n",
        "    similarities = df[\"embedding\"].apply(lambda emb: util.cos_sim(cap_embed, emb).item())\n",
        "    best_match = similarities.idxmax()\n",
        "\n",
        "    matched_data.append({\n",
        "        \"image_path\": path,\n",
        "        \"caption\": caption,\n",
        "        \"matched_title\": df.loc[best_match, \"title\"],\n",
        "        \"keywords\": df.loc[best_match, \"keywords\"],\n",
        "        \"description\": df.loc[best_match, \"description\"]\n",
        "    })\n",
        "\n",
        "matched_df = pd.DataFrame(matched_data)\n"
      ],
      "outputs": []
    },
    {
      "id": "3a08eb9c",
      "cell_type": "code",
      "metadata": {
        "id": "3a08eb9c"
      },
      "execution_count": null,
      "source": [
        "matched_df.to_csv(\"matched_inventory.csv\", index=False)\n",
        "print(\"‚úÖ Exported matched inventory with captions and titles.\")\n",
        "matched_df.head()"
      ],
      "outputs": []
    }
  ]
}